{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import scaper\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import Thesis_models\n",
    "import Thesis_models_nonTime\n",
    "\n",
    "sr = 24000\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESED_classes = ['Alarm_bell_ringing', 'Blender', 'Cat', 'Dishes', 'Dog', 'Electric_shaver_toothbrush', 'Frying', 'Running_water', 'Speech', 'Vacuum_cleaner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 10, 4096, 1) (600, 10, 64, 11)\n",
      "['Alarm_bell_ringing' 'Blender' 'Cat' 'Dishes' 'Dog'\n",
      " 'Electric_shaver_toothbrush' 'Frying' 'None' 'Running_water' 'Speech'\n",
      " 'Vacuum_cleaner']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scene_path = '/Volumes/Expansion/DESED/Dataset_Output_Folder/audio/train/synthetic21_train/soundscapes/'\n",
    "#uni_name = 'soundscape_unimodal' \n",
    "scene_size = sr*10\n",
    "\n",
    "# Create dict for labels\n",
    "\n",
    "#labels_dict = {'non-vehicle':0, 'cars':1, 'motorcycles':2, 'tanks':3, 'trucks':4}\n",
    "#labels_names = ['no_event','cars', 'motorcycles', 'tanks', 'trucks']\n",
    "\n",
    "def audio_input_shaper(path, available_ids):\n",
    "    audio_input = (np.zeros(1,10,4096,1))\n",
    "    for i in available_ids:\n",
    "        audio, _ = librosa.load(path + str(i)+'.wav', sr=sr, mono=True)\n",
    "        x = np.pad(audio, (2861,2898))\n",
    "        x = np.reshape(x, (6,10,4096,1))\n",
    "        audio_input = np.concatenate((audio_input, x), axis=0)\n",
    "\n",
    "    audio_input = audio_input[1,:]\n",
    "\n",
    "    return audio_input\n",
    "\n",
    "n_soundscapes = 100\n",
    "\n",
    "#features = np.zeros((1,10,4096,1))\n",
    "#labels = np.zeros((1, 10, 1))\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for scns in range(n_soundscapes):\n",
    "    scene, _ = librosa.load(scene_path + str(scns) + '.wav', sr=sr, mono=True)\n",
    "    \n",
    "    x1 = np.pad(scene, (2861,2898))\n",
    "    x = np.reshape(x1, (6,10,4096,1))\n",
    "    features.append(x)\n",
    "    #features = np.concatenate((features, x), axis=0)\n",
    "\n",
    "    headers = ['onset', 'offset', 'label']\n",
    "    txt = pd.read_csv(scene_path + str(scns) + '.txt', sep='\\t',  names=headers, index_col=False)\n",
    "\n",
    "    # split scene in chunks for labels\n",
    "    chunk_size = 4096\n",
    "    time_step = 64\n",
    "    step_size = chunk_size//time_step\n",
    "    \n",
    "    # Iterating through the whole sound scene capturing labels\n",
    "    for i in range(0, x1.size, step_size):\n",
    "        labels_temp = []\n",
    "        for b, row in txt.iterrows():\n",
    "            onset = int(row['onset'])*sr\n",
    "            offset = int(row['offset'])*sr\n",
    "            if onset < (i+step_size) and offset > (i):\n",
    "                labels_temp.append(row['label'])\n",
    "            else:\n",
    "                pass    \n",
    "    \n",
    "        for label in labels_temp:\n",
    "            if label in DESED_classes:\n",
    "\n",
    "                pass\n",
    "            else:\n",
    "                labels_temp = ['None']\n",
    "        labels.append(labels_temp)\n",
    "        \n",
    "\n",
    "features_out = np.concatenate(features, axis=0)\n",
    "one_hot = MultiLabelBinarizer()\n",
    "labels = one_hot.fit_transform(labels) \n",
    "label_names = one_hot.classes_\n",
    "labels = np.reshape(labels, ((n_soundscapes*6),10, 64, len(label_names)))\n",
    "\n",
    "print(features_out.shape, labels.shape)\n",
    "print(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRNN_RAe2e = Thesis_models.CRNN(len(label_names), 128, [5, 2, 2], [64, 64], [32], 10, 4096, 128, 64, sr=sr, melspec=False)\n",
    "CRNN_RAe2e.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 10, 4096, 1)]     0         \n",
      "                                                                 \n",
      " conv (TimeDistributed)      (None, 10, 4096, 128)     8320      \n",
      "                                                                 \n",
      " conv_activation (TimeDistri  (None, 10, 4096, 128)    0         \n",
      " buted)                                                          \n",
      "                                                                 \n",
      " conv_smoothing (TimeDistrib  (None, 10, 4096, 128)    16512     \n",
      " uted)                                                           \n",
      "                                                                 \n",
      " conv_smoothing_activation (  (None, 10, 4096, 128)    0         \n",
      " TimeDistributed)                                                \n",
      "                                                                 \n",
      " max_pooling (TimeDistribute  (None, 10, 64, 128)      0         \n",
      " d)                                                              \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 640, 128)          0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 640, 128)         98816     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " LSTM_1 (LSTM)               (None, 640, 64)           49408     \n",
      "                                                                 \n",
      " LSTM_2 (LSTM)               (None, 640, 64)           33024     \n",
      "                                                                 \n",
      " Dense_Xtra (Dense)          (None, 640, 32)           2080      \n",
      "                                                                 \n",
      " Dense_layer (Dense)         (None, 640, 11)           363       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 208,523\n",
      "Trainable params: 208,523\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "LSTM_RAe2e = Thesis_models.LSTM_backend_2(10, 4096, 128, 64, n_of_classes=len(label_names), frame_level_classification=False, output_dim=64, melspec=False, sr=sr)\n",
    "LSTM_RAe2e.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(CRNN_RAe2e, 'CRNN_RAe2e.png', show_shapes=True, show_layer_names=True, expand_nested=True, dpi=396)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_Mel = Thesis_models.LSTM_backend(10, 4096, 128, 64, n_of_classes=len(label_names), frame_level_classification=False, output_dim=64, melspec=True, sr=sr)\n",
    "LSTM_Mel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-18 11:18:48.585070: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-01-18 11:18:48.585951: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer BiLSTM will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer BiLSTM will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer BiLSTM will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer LSTM_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer LSTM_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 10, 4096, 1)]     0         \n",
      "                                                                 \n",
      " conv (TimeDistributed)      (None, 10, 4096, 128)     8320      \n",
      "                                                                 \n",
      " conv_activation (TimeDistri  (None, 10, 4096, 128)    0         \n",
      " buted)                                                          \n",
      "                                                                 \n",
      " conv_smoothing (TimeDistrib  (None, 10, 4096, 128)    16512     \n",
      " uted)                                                           \n",
      "                                                                 \n",
      " conv_smoothing_activation (  (None, 10, 4096, 128)    0         \n",
      " TimeDistributed)                                                \n",
      "                                                                 \n",
      " max_pooling (TimeDistribute  (None, 10, 64, 128)      0         \n",
      " d)                                                              \n",
      "                                                                 \n",
      " BiLSTM (TimeDistributed)    (None, 10, 64, 128)       98816     \n",
      "                                                                 \n",
      " LSTM1 (TimeDistributed)     (None, 10, 64, 64)        49408     \n",
      "                                                                 \n",
      " LSTM2 (TimeDistributed)     (None, 10, 64, 64)        33024     \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 10, 64, 32)       2080      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 10, 64, 11)       363       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 208,523\n",
      "Trainable params: 208,523\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "LSTM_RAe2e = Thesis_models.LSTM_backend(10, 4096, 128, 64, n_of_classes=len(label_names), frame_level_classification=False, output_dim=64, melspec=False, sr=sr)\n",
    "LSTM_RAe2e.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(LSTM_Mel, 'LSTM_Mel.png', show_shapes=True, show_layer_names=True, expand_nested=True, dpi=396)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 10, 4096, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "# forgot the random seed here so the +10 epoch evals are no good...\n",
    "feat_train, feat_test, lab_train, lab_test = train_test_split(features_out, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(feat_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 11)\n"
     ]
    }
   ],
   "source": [
    "#  Run this cell if using RNN layers which are not timedistributed (LSTM_backend_2)\n",
    "\n",
    "lab_train_ = np.reshape(lab_train, (lab_train.shape[0],(lab_train.shape[1]*lab_train.shape[2]),len(label_names)))\n",
    "print(lab_train_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-18 22:27:00.829420: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "\n",
    "callback_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=4, restore_best_weights=True)\n",
    "\n",
    "history = LSTM_RAe2e.fit(\n",
    "    feat_train,\n",
    "    lab_train_,\n",
    "    validation_split=0.2,\n",
    "    #class_weight= {0: 1.0, 1: 1.0, 2: 0.5, 3: 0.5, 4: 1.0},\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    epochs=1,\n",
    "    callbacks = [callback_stop]\n",
    "    \n",
    ")\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 640, 5)\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6042 - accuracy: 0.4883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6041565537452698, 0.48828125]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_test_ = np.reshape(lab_test, (lab_test.shape[0],(lab_test.shape[1]*lab_test.shape[2]),len(label_names)))\n",
    "print(lab_test_.shape)\n",
    "#feat_test_ = np.reshape(feat_test, (120,4096,1))\n",
    "#lab_test_ = np.reshape(lab_test, (120,64,5))\n",
    "\n",
    "LSTM_RAe2e.evaluate(feat_test, lab_test_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CRNN_Rafe 1 epoch 100 soundscapes (approx 12 minutes): loss: 0.2611 - accuracy: 0.3840\n",
    "* CRNN_RAe2e 1 ep 100sc (fixed LR, rnns:[64,64]): [0.2245444357395172, 0.4411458373069763]\n",
    "* LSTM_Rafe 1 epoch 100 soundscapes (approx 9 min): loss: 0.2174 - accuracy: 0.5179\n",
    "* LSTM RAe2e 1 ep 100s (fixed LR no sched) : loss: 0.2451 - accuracy: 0.4411\n",
    "* LSTM_mel 10 epochs 100 soundscapes (approx 7s pr epoch): loss: 0.2120 - accuracy: 0.4411 (Didn't seem to learn, loss decreased but accuracy remained the same)\n",
    "* CRNN_Mel 10 epochs 100 soundscapes (approx 2:30 min pr epoch): loss: 0.2177 - accuracy: 0.4411 (Didn't seem to learn. Guess the log mels are blank...)\n",
    "* CRNN_Mel 2 eps 100s (mels flipped 90 degrees): loss: 0.2165 - accuracy: 0.4411\n",
    "\n",
    "NO TIME DISTRIBUTION TESTS:\n",
    "* LSTM_RAe2e 10 epochs (10 soundscapes)(1 minute each) shuffle=True: [0.1243598461151123, 0.4678385555744171]\n",
    "* LSTM_RAe2e 10 epochs (10 soundscapes) shuffle=False: [0.15079300105571747, 0.4694010317325592] (Did not \"learn\", loss and accuracy almost identical all epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/arvidfalch/Documents/Thesis/Idfrontend2/RAe2e/Models/'\n",
    "model_description = '10epochs_BiLSTM_32_128shape_xtraDense_multi_ground_5class'\n",
    "tf.keras.models.save_model(model, path + model_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = tf.keras.models.load_model('1d5epochs2classesscraper_new_metrics_binaryc_loss')\n",
    "import librosa\n",
    "#path = '/Users/arvidfalch/repos/scaper_test/soundscapes/soundscape_unimodal7.wav'\n",
    "# stab\n",
    "#path = '/Users/arvidfalch/Desktop/BLLTImpt_Bullets_Impact_Body_Thump_Soundly_SND6360.wav'\n",
    "# tanks\n",
    "#path = '/Users/arvidfalch/Desktop/NoCatID_War_Heavy_Trucks_Passing_By_Close_Multiple_Multiple_Heavy_Trucks_Pass_0001_by_JoniHeinonen_Id_161880_JoniHeinonen_None.wav'\n",
    "#path = '/Users/arvidfalch/repos/sq/machine-learning-data/benchmark/dvc-machine-learning-vehicle-data/2021-12-14_Rena_Panzerwagen_hx128ip-00053/2021-12-14T11.09.34UTC_to_2021-12-14T11.10.32UTC_3c2be265/gain_39dB/label_type_track_0.wav'\n",
    "\n",
    "# Cars\n",
    "#path = '/Users/arvidfalch/repos/sq/machine-learning-data/benchmark/dvc-machine-learning-vehicle-data/2021-05-10 car record session us/Sqbundles/2021-05-10T20.23.13UTC_Unknown_US_HX128IP-00014_sounds of things.vehicle.motor vehicle (road).car_from_0.00s_to_3.85s/gain_39dB/label_type_track_0.wav'\n",
    "#path = '/Users/arvidfalch/repos/sq/machine-learning-data/benchmark/dvc-machine-learning-vehicle-data/2022-07-01_Gaustad_Oslo/2022-07-01T08.54.09UTC_to_2022-07-01T08.54.17UTC_5b95032d/gain_39dB/label_type_track_0.wav'\n",
    "# Deep bass\n",
    "#path = '/Users/arvidfalch/Desktop/DSGNRmbl_Designed_Rumble_Bass_Hit_Soundly_SND42461.wav'\n",
    "#path = '/Users/arvidfalch/Desktop/NoCatID_Sweep02_by_833_45_Id_9370_833_45_None.wav'\n",
    "#path = '/Users/arvidfalch/Desktop/NoCatID_Long_Cinematic_Sweep_by_stair_Id_156862_stair_None.wav'\n",
    "#path = '/Users/arvidfalch/Desktop/stockhausen_synth_wav.wav'\n",
    "#path = '/Users/arvidfalch/repos/Vehicles/Air/Drones/Aircraft_Radio_Controlled_Drone_Start_Engines_Slow_Not_Lifting_Off_Then_Stop_Syma_X5SW_SND29303.wav'\n",
    "#path = '/Users/arvidfalch/repos/arvidfalch/Dataset2/2019-03-20_rangetest/2019-03-20T13.34.02UTC_Vestby_R128-1-00002_Rangetest - preflight_from_0.13s_to_38.97s/gain_39dB/label_type_track_2.wav'\n",
    "# drone\n",
    "#path = '/Users/arvidfalch/repos/scaper_test/foreground/drone/Drone112.wav'\n",
    "#path = '/Users/arvidfalch/repos/scaper_test/foreground/drone/Drone145.wav'\n",
    "# Chatting cafe\n",
    "path = '/Users/arvidfalch/repos/Vehicles/Non-Vehicle/chatting cafes/Crowds_Walla_Hotel_Seminar_Room_Interior_Royal_Savoy_Lausanne_200_People_Talking_Business_Hours_SND63966.wav'\n",
    "# Drone upclose flying away\n",
    "#path = '/Users/arvidfalch/repos/scaper_test/foreground/drone/Drone150.wav'\n",
    "# Drone flying off far away\n",
    "#path = '/Users/arvidfalch/repos/scaper_test/foreground/drone/Drone154.wav'\n",
    "# drone\n",
    "#path = '/Users/arvidfalch/repos/scaper_test/soundscapes/soundscape_unimodal4.wav'\n",
    "# short drone\n",
    "#path = '/Users/arvidfalch/repos/scaper_test/soundscapes/soundscape_unimodal7.wav'\n",
    "# Very high frequencies\n",
    "#path = '/Users/arvidfalch/Desktop/NoCatID_Test_Tones_by_acclivity_Id_20680_acclivity_None.wav'\n",
    "# Freq Sweep\n",
    "#path = '/Users/arvidfalch/Desktop/NoCatID_Sine_Sweep_20_Hz_To_20_Khz_10_Seconds_by_reaktorplayer_Id_94224_reaktorplayer_None.wav'\n",
    "# Hydro\n",
    "#path = '/Users/arvidfalch/Desktop/point_az_0.5_el_-5.5.wav'\n",
    "\n",
    "signal, _ = librosa.load(path, sr = sr, mono=True)\n",
    "if signal.size < sr*10:\n",
    "  signal = np.pad(signal, (0, sr*10-signal.size+1))\n",
    "\n",
    "signal = signal[0:240001]\n",
    "x1 = np.pad(signal, (2861,2898))\n",
    "\n",
    "test = np.reshape(x1, (6,10,4096,1))\n",
    "layer_outputs = [layer.output for layer in model.layers[:]]\n",
    "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
    "activations = activation_model.predict(test)\n",
    "  \n",
    "layer_names = []\n",
    "  \n",
    "for layer in model.layers[:]:\n",
    "  layer_names.append(layer.name)\n",
    "print(layer_names)\n",
    "print(len(layer_names))\n",
    "print(layer_names[5])\n",
    "cont = activations[5]\n",
    "\n",
    "cont_ = np.array([])\n",
    "print(cont.shape)\n",
    "for q in range(cont.shape[0]):\n",
    "  for w in range(cont.shape[1]):\n",
    "    cont_ = np.append(cont_, cont[q,w,:])\n",
    "new_shape = int(cont.shape[0]*cont.shape[1]*cont.shape[2])\n",
    "cont_ = cont_.reshape((new_shape, 128))\n",
    "print(cont_.shape)\n",
    "cont_ = cont_.transpose([1,0])\n",
    "\n",
    "y = signal\n",
    "y_pred = model.predict(test)\n",
    "\n",
    "yy = tf.squeeze(y)\n",
    "yy_pred = tf.squeeze(y_pred)\n",
    "\n",
    "import librosa.display\n",
    "x = signal\n",
    "hop_length = 256\n",
    "fig, ax = plt.subplots(figsize= (16,14), nrows=2, ncols=1, sharex=False)\n",
    "\n",
    "\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(x, hop_length=hop_length)),\n",
    "\n",
    "                            ref=np.max)\n",
    "\n",
    "librosa.display.specshow(D, y_axis='log', sr=sr, hop_length=hop_length,\n",
    "\n",
    "                         x_axis='time', ax=ax[0])\n",
    "\n",
    "\n",
    "ax[0] = plt.imshow(cont_, cmap ='viridis', interpolation='nearest', aspect='auto') \n",
    "\n",
    "ax[1].set(title='Log-frequency power spectrogram (upper) vs latent space learned representation')\n",
    "\n",
    "ax[1].label_outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer_test = np.array([activations[-1]])\n",
    "mean_array = np.round(np.mean(last_layer_test,axis=-2),2)\n",
    "print(mean_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1, 6, 10, 6)\n",
    "last_layer_array = np.array([activations[-1]])\n",
    "mean_array = np.round(np.mean(last_layer_array,axis=-2),2)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(np.abs(x1), alpha=0.2)\n",
    "plt.plot(np.repeat(np.ndarray.flatten(mean_array[:,:,:,0]),64*64))\n",
    "plt.plot(np.repeat(np.ndarray.flatten(mean_array[:,:,:,1]),64*64))\n",
    "plt.plot(np.repeat(np.ndarray.flatten(mean_array[:,:,:,2]),64*64))\n",
    "plt.plot(np.repeat(np.ndarray.flatten(mean_array[:,:,:,3]),64*64))\n",
    "plt.plot(np.repeat(np.ndarray.flatten(mean_array[:,:,:,4]),64*64))\n",
    "\n",
    "plt.legend(['audio', 'non_ground','cars', 'motorcycles', 'tanks', 'trucks'], loc='upper right')\n",
    "\n",
    "plt.axhline(0.5, c = 'black')\n",
    "\n",
    "plt.title('Prediction on audio file, mean of predictions (sigmoid 0-1) over 4096 samples windows')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "sf.write('chatter_from_bench.wav', x1, samplerate=24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac1c3d485d1c559db8ec03f0d5193bd9544185f9b671cab351b342fa62eb33ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
